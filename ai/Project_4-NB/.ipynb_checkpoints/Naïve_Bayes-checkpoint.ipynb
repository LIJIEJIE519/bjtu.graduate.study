{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4, Naïve Bayes classifier Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, Implmenting the naïve bayes classifier without using sk-learn.\n",
    "# 2, Implementation need support both discrete and continous features.\n",
    "# 3, classifier evaluation required.\n",
    "# 4, Compare your implementation with Sk-learn regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 连续型数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、数据集载入，划分训练集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 150 samples into 105 train and 45 test samples \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "iris = pd.read_csv('IrisData.csv')\n",
    "def splitData(data_list, ratio):\n",
    "    train_size = int(len(data_list)*ratio)\n",
    "    np.random.shuffle(data_list)\n",
    "    train_set = data_list[:train_size]\n",
    "    test_set = data_list[train_size:]\n",
    "    return train_set,test_set\n",
    "data_list = np.array(iris).tolist()\n",
    "trainset,testset = splitData(data_list,ratio = 0.7)\n",
    "print('Split {0} samples into {1} train and {2} test samples '.format(len(data_list), len(trainset), len(testset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、计算先验概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'virginica': 36, 'setosa': 34, 'versicolor': 35}\n"
     ]
    }
   ],
   "source": [
    "def seprateByClass(dataset):\n",
    "  seprate_dict = {}\n",
    "  info_dict = {}\n",
    "  for vector in dataset:\n",
    "      if vector[-1] not in seprate_dict:\n",
    "          seprate_dict[vector[-1]] = []\n",
    "          info_dict[vector[-1]] = 0\n",
    "      seprate_dict[vector[-1]].append(vector)\n",
    "      info_dict[vector[-1]] +=1\n",
    "  return seprate_dict,info_dict\n",
    "#两个返回值分别为划分好的数据字典，以及划分好的数据集中每个类别的样本数\n",
    "train_separated,train_info = seprateByClass(trainset)\n",
    "print(train_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'virginica': 0.34285714285714286,\n",
       " 'setosa': 0.3238095238095238,\n",
       " 'versicolor': 0.3333333333333333}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算属于每个类别的先验概率\n",
    "\n",
    "def calulateClassPriorProb(dataset,dataset_info):\n",
    "  dataset_prior_prob = {}\n",
    "  sample_sum = len(dataset)\n",
    "  for class_value, sample_nums in dataset_info.items():\n",
    "      dataset_prior_prob[class_value] = sample_nums/float(sample_sum)\n",
    "  return dataset_prior_prob\n",
    "\n",
    "prior_prob = calulateClassPriorProb(trainset,train_info)\n",
    "prior_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、计算类条件概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# 均值\n",
    "def mean(list):\n",
    "  list = [float(x) for x in list] #字符串转数字\n",
    "  return sum(list)/float(len(list))\n",
    "# 方差\n",
    "def var(list):\n",
    "  list = [float(x) for x in list]\n",
    "  avg = mean(list)\n",
    "  var = sum([math.pow((x-avg),2) for x in list])/float(len(list)-1)\n",
    "  return var\n",
    "# 概率密度函数\n",
    "def calculateProb(x,mean,var):\n",
    "    exponent = math.exp(math.pow((x-mean),2)/(-2*var))\n",
    "    p = (1/math.sqrt(2*math.pi*var))*exponent\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5.843809523809524, 0.6786391941391939),\n",
       " (3.038095238095238, 0.1675732600732602),\n",
       " (3.797142857142855, 3.078164835164835),\n",
       " (1.2209523809523808, 0.5859029304029305)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算每个属性的均值和方差\n",
    "def summarizeAttribute(dataset):\n",
    "    dataset = np.delete(dataset,-1,axis = 1) # delete label\n",
    "    summaries = [(mean(attr),var(attr)) for attr in zip(*dataset)]\n",
    "    return summaries\n",
    "\n",
    "summary = summarizeAttribute(trainset)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'virginica': [(6.527777777777778, 0.40492063492063485),\n",
       "  (2.9555555555555553, 0.10825396825396827),\n",
       "  (5.499999999999998, 0.30571428571428577),\n",
       "  (2.030555555555555, 0.08732539682539685)],\n",
       " 'setosa': [(4.988235294117646, 0.12955436720142602),\n",
       "  (3.385294117647059, 0.12432263814616758),\n",
       "  (1.447058823529412, 0.033475935828877),\n",
       "  (0.24705882352941178, 0.012869875222816398)],\n",
       " 'versicolor': [(5.9714285714285715, 0.28915966386554626),\n",
       "  (2.785714285714285, 0.08714285714285715),\n",
       "  (4.328571428571429, 0.18445378151260505),\n",
       "  (1.334285714285714, 0.034084033613445384)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#按类别提取属性特征，这里会得到 类别数目*属性数目 组 （均值，方差）\n",
    "def summarizeByClass(dataset):\n",
    "  dataset_separated,dataset_info = seprateByClass(dataset)\n",
    "  summarize_by_class = {}\n",
    "  for classValue, vector in dataset_separated.items():\n",
    "      summarize_by_class[classValue] = summarizeAttribute(vector)\n",
    "  return summarize_by_class\n",
    "\n",
    "train_Summary_by_class = summarizeByClass(trainset)\n",
    "train_Summary_by_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'virginica': 6.277087462572083e-09,\n",
       " 'setosa': 3.226696701399974,\n",
       " 'versicolor': 1.3732848984171491e-08}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#按类别将每个属性的条件概率相乘。\n",
    "\n",
    "#我们前面已经将训练数据集按类别分好，这里就可以实现，输入的测试数据依据每类的每个属性（就那个类别数*属性数的字典）计算属于某类的类条件概率。\n",
    "def calculateClassProb(input_data,train_Summary_by_class):\n",
    "  prob = {}\n",
    "  for class_value, summary in train_Summary_by_class.items():\n",
    "      prob[class_value] = 1\n",
    "      for i in range(len(summary)):\n",
    "          mean,var = summary[i]\n",
    "          x = input_data[i]\n",
    "          p = calculateProb(x,mean,var)\n",
    "      prob[class_value] *=p\n",
    "  return prob\n",
    "\n",
    "input_vector = testset[1]\n",
    "input_data = input_vector[:-1]\n",
    "train_Summary_by_class = summarizeByClass(trainset)\n",
    "class_prob = calculateClassProb(input_data,train_Summary_by_class)\n",
    "class_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、先验概率*类条件概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#朴素贝叶斯分类器\n",
    "def bayesianPredictOneSample(input_data):\n",
    "  prior_prob = calulateClassPriorProb(trainset,train_info)\n",
    "  train_Summary_by_class = summarizeByClass(trainset)\n",
    "  classprob_dict = calculateClassProb(input_data,train_Summary_by_class)\n",
    "  result = {}\n",
    "  for class_value,class_prob in classprob_dict.items():\n",
    "      p = class_prob*prior_prob[class_value]\n",
    "      result[class_value] = p\n",
    "  return max(result,key=result.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、单个样本测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.9, 2.4, 3.3, 1.0, 'versicolor'], [4.6, 3.2, 1.4, 0.2, 'setosa'], [4.8, 3.4, 1.6, 0.2, 'setosa'], [5.6, 2.9, 3.6, 1.3, 'versicolor'], [5.5, 2.3, 4.0, 1.3, 'versicolor'], [5.4, 3.7, 1.5, 0.2, 'setosa'], [4.8, 3.0, 1.4, 0.3, 'setosa'], [6.6, 3.0, 4.4, 1.4, 'versicolor'], [6.9, 3.1, 5.4, 2.1, 'virginica'], [5.8, 2.7, 4.1, 1.0, 'versicolor'], [6.4, 3.2, 4.5, 1.5, 'versicolor'], [6.1, 2.8, 4.0, 1.3, 'versicolor'], [4.4, 3.2, 1.3, 0.2, 'setosa'], [5.9, 3.2, 4.8, 1.8, 'versicolor'], [6.5, 3.0, 5.5, 1.8, 'virginica'], [5.5, 4.2, 1.4, 0.2, 'setosa'], [5.2, 4.1, 1.5, 0.1, 'setosa'], [5.0, 3.5, 1.3, 0.3, 'setosa'], [6.2, 2.2, 4.5, 1.5, 'versicolor'], [5.6, 2.7, 4.2, 1.3, 'versicolor'], [5.7, 2.6, 3.5, 1.0, 'versicolor'], [5.4, 3.9, 1.7, 0.4, 'setosa'], [4.8, 3.4, 1.9, 0.2, 'setosa'], [4.9, 3.0, 1.4, 0.2, 'setosa'], [6.3, 2.9, 5.6, 1.8, 'virginica'], [4.9, 3.1, 1.5, 0.2, 'setosa'], [5.1, 2.5, 3.0, 1.1, 'versicolor'], [7.3, 2.9, 6.3, 1.8, 'virginica'], [6.0, 3.4, 4.5, 1.6, 'versicolor'], [6.4, 2.7, 5.3, 1.9, 'virginica'], [5.0, 3.4, 1.6, 0.4, 'setosa'], [7.4, 2.8, 6.1, 1.9, 'virginica'], [5.7, 4.4, 1.5, 0.4, 'setosa'], [6.5, 3.0, 5.2, 2.0, 'virginica'], [6.3, 2.3, 4.4, 1.3, 'versicolor'], [5.1, 3.4, 1.5, 0.2, 'setosa'], [6.4, 3.2, 5.3, 2.3, 'virginica'], [6.7, 3.3, 5.7, 2.1, 'virginica'], [5.9, 3.0, 5.1, 1.8, 'virginica'], [6.7, 3.3, 5.7, 2.5, 'virginica'], [5.2, 3.4, 1.4, 0.2, 'setosa'], [6.1, 2.8, 4.7, 1.2, 'versicolor'], [5.8, 2.7, 5.1, 1.9, 'virginica'], [7.7, 2.6, 6.9, 2.3, 'virginica'], [7.9, 3.8, 6.4, 2.0, 'virginica']]\n",
      "the sameple is predicted to class: setosa.\n"
     ]
    }
   ],
   "source": [
    "print(testset)\n",
    "input_vector = testset[2]\n",
    "input_data = input_vector[:-1]\n",
    "result = bayesianPredictOneSample(input_data)\n",
    "print(\"the sameple is predicted to class: {0}.\".format(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculateAccByBeyesian(dataset):\n",
    "  correct = 0\n",
    "  for vector in dataset:\n",
    "      input_data = vector[:-1]\n",
    "      label = vector[-1]\n",
    "      result = bayesianPredictOneSample(input_data)\n",
    "      if result == label:\n",
    "          correct+=1\n",
    "  return correct/len(dataset)\n",
    "\n",
    "acc = calculateAccByBeyesian(testset)\n",
    "acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sk-learn库实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练准确率:0.9481481481481482\n",
      "测试准确率:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import naive_bayes # 导入朴素贝叶斯\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset,testset = train_test_split(iris,test_size=0.1)\n",
    "trainX = trainset.drop('label', axis=1) # 提取数据\n",
    "trainY = trainset['label'] # 提取标签\n",
    "testX = testset.drop('label', axis=1)\n",
    "testY = testset['label']\n",
    "\n",
    "\n",
    "# 建立高斯分布的朴素贝叶斯模型\n",
    "#clf=naive_bayes.GaussianNB()  #高斯分布，没有参数\n",
    "# 建立多项式分布的朴素贝叶斯模型\n",
    "clf=naive_bayes.MultinomialNB()  #多项式分布\n",
    "\n",
    "# 训练模型\n",
    "clf.fit(trainX,trainY)\n",
    "# 输出准确率\n",
    "print(\"训练准确率:\" + str(clf.score(trainX,trainY)))\n",
    "print (\"测试准确率:\" + str(clf.score(testX,testY)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 离散型数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、数据集载入，划分训练集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width       label\n",
      "130           7.4          2.8           6.1          1.9   virginica\n",
      "35            5.0          3.2           1.2          0.2      setosa\n",
      "74            6.4          2.9           4.3          1.3  versicolor\n",
      "66            5.6          3.0           4.5          1.5  versicolor\n",
      "2             4.7          3.2           1.3          0.2      setosa\n",
      "..            ...          ...           ...          ...         ...\n",
      "51            6.4          3.2           4.5          1.5  versicolor\n",
      "54            6.5          2.8           4.6          1.5  versicolor\n",
      "43            5.0          3.5           1.6          0.6      setosa\n",
      "112           6.8          3.0           5.5          2.1   virginica\n",
      "6             4.6          3.4           1.4          0.3      setosa\n",
      "\n",
      "[105 rows x 5 columns]\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#car = pd.read_csv(\"CarEvalution.csv\")\n",
    "iris = pd.read_csv('IrisData.csv')\n",
    "iris_train,iris_test = train_test_split(iris,test_size=0.3)\n",
    "print(iris_train)\n",
    "print(len(iris_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、获取相关参数 getParams(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versicolor    36\n",
      "setosa        35\n",
      "virginica     34\n",
      "Name: label, dtype: int64 ['versicolor' 'setosa' 'virginica'] 105 3\n"
     ]
    }
   ],
   "source": [
    "def getParams(data):\n",
    "        ck_counts = data.label.value_counts()#训练样本中类为ck的数量集合\n",
    "        ck_name = np.array(ck_counts.index)#训练样本中类ck名称集合    \n",
    "        DataNum = len(data)#训练样本总数N\n",
    "        CNum = len(ck_counts)#类的个数K\n",
    "        DataSet = data\n",
    "        return ck_counts,ck_name,DataNum,CNum\n",
    "DataSet = iris_train\n",
    "ck_counts,ck_name,DataNum,CNum = getParams(iris_train)\n",
    "print(ck_counts,ck_name,DataNum,CNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、计算先验概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3425925925925926, 0.3333333333333333, 0.32407407407407407]\n"
     ]
    }
   ],
   "source": [
    "def calPriorProb(CNum,ck_counts,DataNum,lamda):\n",
    "    ck_PriorProb = []\n",
    "    for i in range(CNum):\n",
    "        cx_PriorProb = (ck_counts[i]+lamda)/(DataNum+CNum*lamda)\n",
    "        ck_PriorProb.append(cx_PriorProb)\n",
    "    return ck_PriorProb   \n",
    "ck_PriorProb = calPriorProb(CNum,ck_counts,DataNum,lamda=1)\n",
    "print(calPriorProb(CNum,ck_counts,DataNum,lamda=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、计算类条件概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0         1         2         3         4         5    \\\n",
      "versicolor  0.037037  0.055556  0.092593  0.037037  0.037037  0.111111   \n",
      "setosa      0.145833  0.020833  0.020833  0.104167  0.125000  0.020833   \n",
      "virginica   0.018868  0.113208  0.056604  0.037736  0.018868  0.037736   \n",
      "\n",
      "                 6         7         8         9    ...       106       107  \\\n",
      "versicolor  0.055556  0.055556  0.074074  0.055556  ...  0.022727  0.022727   \n",
      "setosa      0.041667  0.020833  0.020833  0.020833  ...  0.024390  0.097561   \n",
      "virginica   0.056604  0.075472  0.056604  0.056604  ...  0.086957  0.021739   \n",
      "\n",
      "                 108       109       110       111       112       113  \\\n",
      "versicolor  0.068182  0.068182  0.022727  0.022727  0.022727  0.022727   \n",
      "setosa      0.024390  0.024390  0.024390  0.024390  0.024390  0.048780   \n",
      "virginica   0.021739  0.021739  0.065217  0.043478  0.043478  0.021739   \n",
      "\n",
      "                 114       115  \n",
      "versicolor  0.022727  0.022727  \n",
      "setosa      0.024390  0.048780  \n",
      "virginica   0.043478  0.021739  \n",
      "\n",
      "[3 rows x 116 columns]\n"
     ]
    }
   ],
   "source": [
    "def calCondProb(ck_name,DataSet,CNum,lamda):\n",
    "    names = locals()#使用动态变量\n",
    "    CondProb = []#存储所有类别的所有特征取值的条件概率\n",
    "    feat_value = []#所有特征取值列表\n",
    "        \n",
    "        #对于每一类别的数据集\n",
    "    for i in range(len(ck_name)):\n",
    "        names['Q%s' % i] = DataSet[DataSet[\"label\"]==ck_name[i]]#按类别划分数据集\n",
    "        names['ConProbC%s' % i] = []#定义动态变量，表示各类别中所有特征取值的条件概率集合\n",
    "        feature_arr = DataSet.columns.tolist()[0:len(DataSet.columns)-1]#获取训练数据集特征集\n",
    "          \n",
    "            #对于每一个特征求该特征各个取值的条件概率\n",
    "        for feature in (feature_arr):\n",
    "            names['Q%s' % feature]=[]#定义动态变量，表示某个类别的某个特征的所有取值条件概率\n",
    "                \n",
    "                #对于某个特征的所有可能取值求条件概率\n",
    "            for value in DataSet[feature].value_counts().index.tolist():\n",
    "                   \n",
    "                    #生成所有特征取值列表\n",
    "                if value not in feat_value:#如果这个取值不在列表中，则加入这个取值\n",
    "                    feat_value.append(value)\n",
    "#                     print(feat_value)\n",
    "                    #这里用了拉普拉斯平滑，使得条件概率不会出现0的情况\n",
    "                    #如果某个类的某个特征取值在训练集上都出现过，则这样计算\n",
    "                if value in names['Q%s' % i][feature].value_counts():\n",
    "                    temp = (names['Q%s' % i][feature].value_counts()[value]+lamda)/(names['Q%s' % i][feature].value_counts().sum()+len(names['Q%s' % i][feature].value_counts())*lamda)\n",
    "                    #如果某个类的某个特征取值并未在训练集上出现，为了避免出现0的情况，分子取1(即lamda平滑因子，取1时为拉普拉斯平滑)\n",
    "                else:\n",
    "                    temp = lamda/(names['Q%s' % i][feature].value_counts().sum()+len(names['Q%s' % i][feature].value_counts())*lamda)    \n",
    "                names['Q%s' % feature].append(temp)#将求得的特征取值条件概率加入列表\n",
    "            names['ConProbC%s' % i].extend(names['Q%s' % feature])#将得到的某个类别的某个特征的所有取值条件概率列表加入某个类别中所有特征取值的条件概率集合\n",
    "        CondProb.append(names['ConProbC%s' % i])#将某个类别中所有特征取值的条件概率集合加入所有类别所有特征取值的条件概率集合\n",
    "#     CondProb.append(feat_value)#将所有特征取值列表也加入所有类别所有特征取值的条件概率集合(后面用来做columns--列索引)\n",
    "    index = ck_name.tolist() #用类别名称的集合来生成行索引index\n",
    "    \n",
    "#     index.extend(['other'])#此处由于我最后一行是feat_value，后面会删掉，因此在行索引上也多加一个，后面删掉\n",
    "    CondProb = pd.DataFrame(CondProb,index)#将所有类别所有特征取值的条件概率集合转换为DataFrame格式\n",
    "#     CondProb.drop(['other'],inplace = True)\n",
    "    return names,CondProb,feat_value\n",
    "names,CondProb,feat_value = calCondProb(ck_name,DataSet,CNum,lamda=1)    \n",
    "print(CondProb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、对一个样本进行预测  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3425925925925926 versicolor\n"
     ]
    }
   ],
   "source": [
    "def predict(traindata,testdata):\n",
    "    getParams(traindata)#获取参数\n",
    "    calPriorProb(CNum,ck_counts,DataNum,lamda=1)#获取先验概率\n",
    "    calCondProb(ck_name,traindata,CNum,lamda=1)#获取条件概率\n",
    "    \n",
    "    ClassTotalProb = []#初始化各类别总概率列表\n",
    "    bestprob = -1#初始化最高概率\n",
    "    bestfeat = ''#初始化最可能类别\n",
    "    \n",
    "    for feat in ck_name:\n",
    "        pp = ck_PriorProb[ck_name.tolist().index(feat)]#pp为先验概率\n",
    "        cp = 1#初始化条件概率\n",
    "        for value in feat_value:\n",
    "            if value in testdata.index.tolist():\n",
    "                cp = cp * CondProb[value][feat]#计算各特征取值的条件概率之积\n",
    "               \n",
    "        TotalProb = pp * cp#条件概率之积与先验概率相乘\n",
    "       \n",
    "        ClassTotalProb.append(TotalProb)\n",
    "    #ClassTotalProb = pd.DataFrame(ClassTotalProb,ck_name)\n",
    "    #print(ClassTotalProb)\n",
    "    \n",
    "        #找到最可能类别和其概率    \n",
    "    for i in range(len(ck_name)):\n",
    "        if ClassTotalProb[i] > bestprob:\n",
    "            bestprob = ClassTotalProb[i]\n",
    "            bestfeat = ck_name[i]\n",
    "    return bestprob,bestfeat\n",
    "bestprob,bestfeat = predict(iris_train,iris_test)\n",
    "print(bestprob,bestfeat)\n",
    "# #print(car_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、计算预测准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor virginica\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor versicolor\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3425925925925926 versicolor setosa\n",
      "0.3111111111111111\n"
     ]
    }
   ],
   "source": [
    "def getAccuracy(traindata,testdata):\n",
    "    num = 0\n",
    "    realFeat = testdata.label.tolist()\n",
    "    \n",
    "    for i in range(len(testdata)):\n",
    "        temp = testdata.iloc[i][0:len(testdata.columns)-1]    \n",
    "        predProb,predFeat = predict(testdata,temp)\n",
    "        print(predProb,predFeat,realFeat[i])\n",
    "        if(realFeat[i] == predFeat):\n",
    "            num = num + 1\n",
    "    acc = num / float(len(realFeat))\n",
    "    return acc\n",
    "acc = getAccuracy(iris_train,iris_test)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
